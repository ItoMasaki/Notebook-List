{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GraspWithYOLOv1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/ItoMasaki/Notebook-List/blob/main/GraspWithYOLOv1.ipynb",
      "authorship_tag": "ABX9TyMpodWU0a7fI4HWt5O0qAor",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b721bfe67bea4cc7874e7c075ebfa3e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6f667266218f487cbc19822566009c4a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4e50f155af7d46bfa80078d6d2de644a",
              "IPY_MODEL_26f56ca2bfaa483bb22bb38aba478209",
              "IPY_MODEL_9fa9685e9795491cb6f433a56c210e2d"
            ]
          }
        },
        "6f667266218f487cbc19822566009c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "4e50f155af7d46bfa80078d6d2de644a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1531d96baab040f99229623acf1f5551",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 299: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f57b48550d1f4d4cba3ff2ad1a1ace40"
          }
        },
        "26f56ca2bfaa483bb22bb38aba478209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ed03e1c7b3f4bc586ba472c3dc68509",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 81,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 81,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_628df46fa5c04a999978bfe0baceebbe"
          }
        },
        "9fa9685e9795491cb6f433a56c210e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0dab0403088467f9c346a8e87914735",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 81/81 [00:15&lt;00:00,  5.32it/s, loss=0.039, v_num=0]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_395a5dc2c95c4640aedca54c0de97834"
          }
        },
        "1531d96baab040f99229623acf1f5551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f57b48550d1f4d4cba3ff2ad1a1ace40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ed03e1c7b3f4bc586ba472c3dc68509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "628df46fa5c04a999978bfe0baceebbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0dab0403088467f9c346a8e87914735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "395a5dc2c95c4640aedca54c0de97834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItoMasaki/Notebook-List/blob/main/GraspWithYOLOv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "OrkeT90t2UCZ"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfY30OLagVEJ",
        "outputId": "e23a7d97-ab21-4c00-c248-be34c4394098"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.5.8)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.7.0)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.1.0)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.43.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "7Be5Tf54qZXc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import re\n",
        "\n",
        "from skimage.io import imread\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "from numpy import array\n",
        "\n",
        "from random import gauss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def atoi(text):    \n",
        "    return int(text) if text.isdigit() else text    \n",
        "    \n",
        "def natural_keys(text):    \n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
      ],
      "metadata": {
        "id": "_JxrUWzaVuDf"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadData():\n",
        "  def __init__(self):\n",
        "    T_annotations_path = \"/content/drive/MyDrive/datasets/image_based_grasp_point/Annotations/*\"\n",
        "    T_images_path = \"/content/drive/MyDrive/datasets/image_based_grasp_point/Images/*\"\n",
        "    \n",
        "    T_annotations_pathes = sorted(glob(T_annotations_path), key=natural_keys)\n",
        "    T_images_pathes = sorted(glob(T_images_path), key=natural_keys)\n",
        "\n",
        "    self.X = []\n",
        "    self.Y = []\n",
        "\n",
        "    self.T_label_data = []\n",
        "    self.T_image_data = []\n",
        "    self.T_distr_data = []\n",
        "\n",
        "    for idx, annotation_path in enumerate(T_annotations_pathes):\n",
        "      tmp_image = array(Image.fromarray(imread(T_images_pathes[idx])))\n",
        "      shape = tmp_image.shape\n",
        "\n",
        "      image = array(Image.fromarray(imread(T_images_pathes[idx])).resize((128, 128)))/255\n",
        "\n",
        "      with open(annotation_path, \"r\") as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        if len(data) == 3:\n",
        "          positions = data[0:2]\n",
        "          dist = eval(data[2])\n",
        "          p_1 = [int(p)/shape[i] for i, p in enumerate(positions[0].replace(\"\\n\", \"\").split(\" \"))]\n",
        "          p_2 = [int(p)/shape[i] for i, p in enumerate(positions[1].replace(\"\\n\", \"\").split(\" \"))]\n",
        "          self.T_label_data.append(array(p_1 + p_2))\n",
        "          self.T_image_data.append(image.transpose(2, 0, 1))\n",
        "          self.T_distr_data.append(array(dist))\n",
        "\n",
        "          for n in range(20):\n",
        "            p_1 = [gauss(int(p)/shape[i], 0.01) for i, p in enumerate(positions[0].replace(\"\\n\", \"\").split(\" \"))]\n",
        "            p_2 = [gauss(int(p)/shape[i], 0.01) for i, p in enumerate(positions[1].replace(\"\\n\", \"\").split(\" \"))]\n",
        "            self.T_label_data.append(array(p_1 + p_2))\n",
        "            self.T_image_data.append(image.transpose(2, 0, 1))\n",
        "            self.T_distr_data.append(array(dist))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return [self.T_image_data[index], self.T_distr_data[index]], self.T_label_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.T_label_data)\n",
        "\n",
        "load_data = LoadData()"
      ],
      "metadata": {
        "id": "4ADJmvysrEni"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraspingPoinntDataModule(pl.LightningDataModule):    \n",
        "    def __init__(self, path, batch_size):    \n",
        "        super().__init__()    \n",
        "    \n",
        "        self.path = path    \n",
        "        self.batch_size = batch_size    \n",
        "    \n",
        "    def setup(self, stage=None):    \n",
        "        self.grasping_point_train = LoadData()    \n",
        "        #self.grasping_point_test = ExpertDataset(self.path, validation_flag=True)    \n",
        "        #self.grasping_point_val = ExpertDataset(self.path, validation_flag=True)    \n",
        "        # self.grasping_point_train, self.grasping_point_val = random_split(grasping_point_full, [len(grasping_point_full)-val_num, val_num])    \n",
        "    \n",
        "    def train_dataloader(self):    \n",
        "        return DataLoader(self.grasping_point_train, batch_size=self.batch_size, num_workers=16, shuffle=True)    \n",
        "    \n",
        "    #def val_dataloader(self):    \n",
        "    #    return DataLoader(self.grasping_point_val, batch_size=len(self.grasping_point_val), num_workers=16, shuffle=False)    \n",
        "    \n",
        "    #def test_dataloader(self):    \n",
        "    #    return DataLoader(self.grasping_point_test, batch_size=len(self.grasping_point_val), shuffle=False)"
      ],
      "metadata": {
        "id": "XnB6kAFYg3f2"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch    \n",
        "from torch import nn    \n",
        "    \n",
        "class ExtractFeature(nn.Module):    \n",
        "    def __init__(self, in_channels=4, out_channels=32, kernel_size=3):    \n",
        "        super().__init__()    \n",
        "    \n",
        "        self.up_channel = nn.Conv2d(3, 4, 4, 2, 1)    \n",
        "    \n",
        "        self.backborn = nn.Sequential(    \n",
        "                nn.Conv2d(in_channels, in_channels*2, kernel_size, 2, 1),    \n",
        "                nn.ReLU(),    \n",
        "                nn.Conv2d(in_channels*2, in_channels*3, kernel_size, 2, 1),    \n",
        "                nn.ReLU(),    \n",
        "                nn.Conv2d(in_channels*3, in_channels*4, kernel_size, 2, 1),    \n",
        "                nn.ReLU(),    \n",
        "                nn.Conv2d(in_channels*4, in_channels*5, kernel_size, 2, 1),    \n",
        "                nn.ReLU(),    \n",
        "                nn.Conv2d(in_channels*5, in_channels*6, kernel_size, 2, 1),    \n",
        "                nn.ReLU(),    \n",
        "                nn.Conv2d(in_channels*6, in_channels*7, kernel_size, 2, 1),    \n",
        "        )    \n",
        "    \n",
        "    def forward(self, x):    \n",
        "        x = self.up_channel(x)    \n",
        "        x = self.backborn(x)    \n",
        "        return x"
      ],
      "metadata": {
        "id": "9sPu7PCAh-Gf"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch    \n",
        "from torch import nn    \n",
        "from torch.utils.tensorboard import SummaryWriter    \n",
        "    \n",
        "    \n",
        "class Encoder(nn.Module):    \n",
        "    def __init__(self, in_channels=28+16, out_channels=32):    \n",
        "        super().__init__()    \n",
        "    \n",
        "        self.encode = nn.Sequential(    \n",
        "                nn.Linear(in_channels, 64),        \n",
        "                nn.SiLU(),        \n",
        "                nn.Linear(64, 56),    \n",
        "                nn.SiLU(),    \n",
        "                nn.Linear(56, 48),    \n",
        "                nn.SiLU(),    \n",
        "                nn.Linear(48, 40),    \n",
        "                nn.SiLU(),    \n",
        "                nn.Linear(40, 32),    \n",
        "                nn.SiLU(),    \n",
        "        )\n",
        "\n",
        "    def forward(self, x):    \n",
        "        x = self.encode(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "NoywabgwiIr1"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels=28+16, out_channels=5):    \n",
        "        super().__init__()    \n",
        "\n",
        "        self.decode = nn.Sequential(\n",
        "                nn.Linear(32, 16),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(16, 8),\n",
        "                nn.SiLU(),\n",
        "        )\n",
        "\n",
        "        self.decode_pos_1 = nn.Sequential(\n",
        "                nn.Linear(8, 6),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(6, 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(4, 2),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.decode_pos_2 = nn.Sequential(    \n",
        "                nn.Linear(8, 6),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(6, 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(4, 2),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.decode(x)\n",
        "\n",
        "        pos_1 = self.decode_pos_1(x)\n",
        "        pos_2 = self.decode_pos_2(x)\n",
        "\n",
        "        x = torch.cat((pos_1, pos_2), 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RLytHCSEiOQE"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.writer = SummaryWriter(log_dir=\"/content/drive/MyDrive/\")\n",
        "\n",
        "    self.extract_feature = ExtractFeature()\n",
        "    self.linear_transformation = nn.Linear(11, 16)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.encode = Encoder()\n",
        "    self.decoder = Decoder()\n",
        "\n",
        "  def forward(self, image, dist):    \n",
        "    feature_from_image =  self.flatten(self.extract_feature(image.float()))    \n",
        "    feature_from_yolo = self.linear_transformation(dist.float())    \n",
        "    feature = torch.cat((feature_from_image, feature_from_yolo), 1)\n",
        "    x = self.encode(feature.float())\n",
        "    x_hat = self.decoder(x)    \n",
        "    return x_hat\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    image, dist = x\n",
        "\n",
        "    x_hat = self(image, dist)\n",
        "\n",
        "    loss = F.mse_loss(x_hat[:, 0:4], y.float()[:, 0:4])\n",
        "\n",
        "    self.log('loss/train', loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "cgYVlJxvisOC"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from models.GraspingNetwork import Network    \n",
        "\n",
        "batch_size = 128\n",
        "max_epochs = 300\n",
        "gpus = 0\n",
        "checkpoint_path = \"/content/lightning_logs/version_10/checkpoints/epoch=210-step=17090.ckpt\"\n",
        "    \n",
        "def main():\n",
        "    tb_logger = pl_loggers.TensorBoardLogger(\"/content/drive/MyDrive/logs\")\n",
        "    GPDataset = GraspingPoinntDataModule(\"data/my_research\", batch_size)\n",
        "    model = Network()\n",
        "\n",
        "    trainer = pl.Trainer(gpus=gpus, max_epochs=max_epochs, logger=tb_logger)\n",
        "\n",
        "    if checkpoint_path is not None:\n",
        "      trainer.fit(model, GPDataset, ckpt_path=checkpoint_path)\n",
        "    else:\n",
        "      trainer.fit(model, GPDataset)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491,
          "referenced_widgets": [
            "b721bfe67bea4cc7874e7c075ebfa3e1",
            "6f667266218f487cbc19822566009c4a",
            "4e50f155af7d46bfa80078d6d2de644a",
            "26f56ca2bfaa483bb22bb38aba478209",
            "9fa9685e9795491cb6f433a56c210e2d",
            "1531d96baab040f99229623acf1f5551",
            "f57b48550d1f4d4cba3ff2ad1a1ace40",
            "3ed03e1c7b3f4bc586ba472c3dc68509",
            "628df46fa5c04a999978bfe0baceebbe",
            "e0dab0403088467f9c346a8e87914735",
            "395a5dc2c95c4640aedca54c0de97834"
          ]
        },
        "id": "2pa_ADCfkVzJ",
        "outputId": "a09d3ffb-26f8-4e2b-b733-e3597ceb3310"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "Restoring states from the checkpoint path at /content/lightning_logs/version_10/checkpoints/epoch=210-step=17090.ckpt\n",
            "Missing logger folder: /content/drive/MyDrive/logs/default\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:248: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.\n",
            "  \"You're resuming from a checkpoint that ended mid-epoch.\"\n",
            "Restored all states from the checkpoint file at /content/lightning_logs/version_10/checkpoints/epoch=210-step=17090.ckpt\n",
            "\n",
            "  | Name                  | Type           | Params\n",
            "---------------------------------------------------------\n",
            "0 | extract_feature       | ExtractFeature | 16.4 K\n",
            "1 | linear_transformation | Linear         | 192   \n",
            "2 | flatten               | Flatten        | 0     \n",
            "3 | encode                | Encoder        | 12.5 K\n",
            "4 | decoder               | Decoder        | 848   \n",
            "---------------------------------------------------------\n",
            "30.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "30.0 K    Total params\n",
            "0.120     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b721bfe67bea4cc7874e7c075ebfa3e1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}