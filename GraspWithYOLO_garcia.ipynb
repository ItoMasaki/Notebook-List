{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItoMasaki/Notebook-List/blob/main/GraspWithYOLO_garcia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHsB9Yq9Kpgw",
        "outputId": "0bab8601-50f2-4963-fb69-59d4c98c569b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s4-ubDNK2rN",
        "outputId": "70340eee-4653-4c22-c4b2-749f6cbf6530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.5.8)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.1.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.1)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.7.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.43.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.10)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uAjPrWrK4hd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "import re\n",
        "\n",
        "from skimage.io import imread\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "from numpy import array\n",
        "\n",
        "from random import gauss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGLyJFiDK8LV"
      },
      "outputs": [],
      "source": [
        "def atoi(text):    \n",
        "    return int(text) if text.isdigit() else text    \n",
        "    \n",
        "def natural_keys(text):    \n",
        "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeVIEE_QK-BW"
      },
      "outputs": [],
      "source": [
        "class LoadData():\n",
        "  def __init__(self):\n",
        "    T_annotations_path = \"/content/drive/MyDrive/datasets/image_based_grasp_point/Annotations/*\"\n",
        "    T_images_path = \"/content/drive/MyDrive/datasets/image_based_grasp_point/Images/*\"\n",
        "    \n",
        "    T_annotations_pathes = sorted(glob(T_annotations_path), key=natural_keys)\n",
        "    T_images_pathes = sorted(glob(T_images_path), key=natural_keys)\n",
        "\n",
        "    self.X = []\n",
        "    self.Y = []\n",
        "\n",
        "    self.T_label_data = []\n",
        "    self.T_image_data = []\n",
        "    self.T_distr_data = []\n",
        "\n",
        "    for idx, annotation_path in enumerate(T_annotations_pathes):\n",
        "      tmp_image = array(Image.fromarray(imread(T_images_pathes[idx])))\n",
        "      shape = tmp_image.shape\n",
        "\n",
        "      image = array(Image.fromarray(imread(T_images_pathes[idx])).resize((128, 128)))/255\n",
        "\n",
        "      with open(annotation_path, \"r\") as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "        if len(data) == 3:\n",
        "          positions = data[0:2]\n",
        "          dist = eval(data[2])\n",
        "          p_1 = [int(p)/shape[i] for i, p in enumerate(positions[0].replace(\"\\n\", \"\").split(\" \"))]\n",
        "          p_2 = [int(p)/shape[i] for i, p in enumerate(positions[1].replace(\"\\n\", \"\").split(\" \"))]\n",
        "          self.T_label_data.append(array(p_1 + p_2))\n",
        "          self.T_image_data.append(image.transpose(2, 0, 1))\n",
        "          self.T_distr_data.append(array(dist))\n",
        "\n",
        "          for n in range(20):\n",
        "            p_1 = [gauss(int(p)/shape[i], 0.01) for i, p in enumerate(positions[0].replace(\"\\n\", \"\").split(\" \"))]\n",
        "            p_2 = [gauss(int(p)/shape[i], 0.01) for i, p in enumerate(positions[1].replace(\"\\n\", \"\").split(\" \"))]\n",
        "            self.T_label_data.append(array(p_1 + p_2))\n",
        "            self.T_image_data.append(image.transpose(2, 0, 1))\n",
        "            self.T_distr_data.append(array(dist))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return [self.T_image_data[index], self.T_distr_data[index]], self.T_label_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.T_label_data)\n",
        "\n",
        "load_data = LoadData()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3IhCf5rLBBH"
      },
      "outputs": [],
      "source": [
        "class GraspingPoinntDataModule(pl.LightningDataModule):    \n",
        "    def __init__(self, path, batch_size):    \n",
        "        super().__init__()    \n",
        "    \n",
        "        self.path = path    \n",
        "        self.batch_size = batch_size    \n",
        "    \n",
        "    def setup(self, stage=None):    \n",
        "        self.grasping_point_train = LoadData()    \n",
        "        #self.grasping_point_test = ExpertDataset(self.path, validation_flag=True)    \n",
        "        #self.grasping_point_val = ExpertDataset(self.path, validation_flag=True)    \n",
        "        # self.grasping_point_train, self.grasping_point_val = random_split(grasping_point_full, [len(grasping_point_full)-val_num, val_num])    \n",
        "    \n",
        "    def train_dataloader(self):    \n",
        "        return DataLoader(self.grasping_point_train, batch_size=self.batch_size, num_workers=16, shuffle=True)    \n",
        "    \n",
        "    #def val_dataloader(self):    \n",
        "    #    return DataLoader(self.grasping_point_val, batch_size=len(self.grasping_point_val), num_workers=16, shuffle=False)    \n",
        "    \n",
        "    #def test_dataloader(self):    \n",
        "    #    return DataLoader(self.grasping_point_test, batch_size=len(self.grasping_point_val), shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad8V5numLDHK"
      },
      "outputs": [],
      "source": [
        "import torch    \n",
        "from torch import nn    \n",
        "    \n",
        "class ExtractFeature(nn.Module):    \n",
        "    def __init__(self, in_channels=4, out_channels=32, kernel_size=3):    \n",
        "        super().__init__()    \n",
        "    \n",
        "        self.up_channel = nn.Conv2d(3, 4, 4, 2, 1)    \n",
        "    \n",
        "        self.backborn = nn.Sequential(    \n",
        "                nn.Conv2d(in_channels, in_channels*2, kernel_size, 2, 1),\n",
        "                nn.Conv2d(in_channels*2, in_channels*3, kernel_size, 2, 1),    \n",
        "                nn.BatchNorm2d(in_channels*3),\n",
        "                nn.Conv2d(in_channels*3, in_channels*4, kernel_size, 2, 1),    \n",
        "                nn.BatchNorm2d(in_channels*4),\n",
        "                nn.Conv2d(in_channels*4, in_channels*5, kernel_size, 2, 1),    \n",
        "                nn.BatchNorm2d(in_channels*5),\n",
        "                nn.Conv2d(in_channels*5, in_channels*6, kernel_size, 2, 1),    \n",
        "                nn.BatchNorm2d(in_channels*6),\n",
        "                nn.Conv2d(in_channels*6, in_channels*7, kernel_size, 2, 1),    \n",
        "        )    \n",
        "    \n",
        "    def forward(self, x):    \n",
        "        x = self.up_channel(x)    \n",
        "        x = self.backborn(x)    \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhG5BttgLE_F"
      },
      "outputs": [],
      "source": [
        "import torch    \n",
        "from torch import nn    \n",
        "from torch.utils.tensorboard import SummaryWriter    \n",
        "    \n",
        "    \n",
        "class Encoder(nn.Module):    \n",
        "    def __init__(self, in_channels=28, out_channels=32):    \n",
        "        super().__init__()    \n",
        "    \n",
        "        self.encode = nn.Sequential(    \n",
        "                nn.Linear(in_channels, 64),        \n",
        "                nn.SiLU(),        \n",
        "                nn.Linear(64, 56),    \n",
        "                nn.SiLU(),    \n",
        "                nn.Linear(56, 48),    \n",
        "                nn.SiLU(),    \n",
        "                nn.Linear(48, 40),    \n",
        "                nn.SiLU(),    \n",
        "                nn.Linear(40, 32),    \n",
        "                nn.SiLU(),    \n",
        "        )\n",
        "\n",
        "    def forward(self, x):    \n",
        "        x = self.encode(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OYHdgLCLG0H"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels=28+16, out_channels=5):    \n",
        "        super().__init__()    \n",
        "\n",
        "        self.decode = nn.Sequential(\n",
        "                nn.Linear(32, 16),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(16, 8),\n",
        "                nn.SiLU(),\n",
        "        )\n",
        "\n",
        "        self.decode_pos_1 = nn.Sequential(\n",
        "                nn.Linear(8, 6),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(6, 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(4, 2),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.decode_pos_2 = nn.Sequential(    \n",
        "                nn.Linear(8, 6),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(6, 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(4, 2),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.decode(x)\n",
        "\n",
        "        pos_1 = self.decode_pos_1(x)\n",
        "        pos_2 = self.decode_pos_2(x)\n",
        "\n",
        "        x = torch.cat((pos_1, pos_2), 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGxIWq8kLIuV"
      },
      "outputs": [],
      "source": [
        "class Network(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.extract_feature = ExtractFeature()\n",
        "    self.linear_transformation = nn.Linear(11, 16)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.encode = Encoder()\n",
        "    self.decoder = Decoder()\n",
        "\n",
        "  def forward(self, image, dist):    \n",
        "    feature_from_image = self.flatten(self.extract_feature(image.float()))\n",
        "    x = self.encode(feature_from_image)\n",
        "    x_hat = self.decoder(x)    \n",
        "    return x_hat\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    image, dist = x\n",
        "\n",
        "    x_hat = self(image, dist)\n",
        "\n",
        "    loss = F.mse_loss(x_hat[:, 0:4], y.float()[:, 0:4])\n",
        "\n",
        "    self.log('loss/train', loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 455,
          "referenced_widgets": [
            "8af95d9551574983a8fcd16fcfc4892a"
          ]
        },
        "id": "emFJnaJ-LKmJ",
        "outputId": "90e4ac7a-4c51-4fc9-ff15-6363c0b464ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "\n",
            "  | Name                  | Type           | Params\n",
            "---------------------------------------------------------\n",
            "0 | extract_feature       | ExtractFeature | 16.6 K\n",
            "1 | linear_transformation | Linear         | 192   \n",
            "2 | flatten               | Flatten        | 0     \n",
            "3 | encode                | Encoder        | 11.5 K\n",
            "4 | decoder               | Decoder        | 848   \n",
            "---------------------------------------------------------\n",
            "29.1 K    Trainable params\n",
            "0         Non-trainable params\n",
            "29.1 K    Total params\n",
            "0.116     Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8af95d9551574983a8fcd16fcfc4892a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# from models.GraspingNetwork import Network    \n",
        "\n",
        "batch_size = 128\n",
        "max_epochs = 300\n",
        "gpus = 1\n",
        "checkpoint_path = None#\"/content/lightning_logs/version_10/checkpoints/epoch=210-step=17090.ckpt\"\n",
        "    \n",
        "def main():\n",
        "    tb_logger = pl_loggers.TensorBoardLogger(\"/content/drive/MyDrive/logs\")\n",
        "    GPDataset = GraspingPoinntDataModule(\"data/my_research\", batch_size)\n",
        "    model = Network()\n",
        "\n",
        "    trainer = pl.Trainer(gpus=gpus, max_epochs=max_epochs, logger=tb_logger)\n",
        "\n",
        "    if checkpoint_path is not None:\n",
        "      trainer.fit(model, GPDataset, ckpt_path=checkpoint_path)\n",
        "    else:\n",
        "      trainer.fit(model, GPDataset)\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GraspWithYOLO_garcia.ipynb",
      "provenance": [],
      "mount_file_id": "11_DTsrVCZ0A1O5W8ROMnapUUoAldpnku",
      "authorship_tag": "ABX9TyMvhkFxp4UtCMVmNjKkRk63",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}